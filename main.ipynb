{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bring in the RLBench BC data from the saved hdf5 data files for each task. The task files were created with the instructRL/data/collect_data.py script. The task files include an array of data samples.  Each sample is a dict that includes the following keys:\n",
    "\n",
    "* image - data for 4 camera positions: front_rgb, left_shoulder_rgb, right_shoulder_rgb, wrist_rgb\n",
    "* instruct - the text instruction\n",
    "* action - the action vector: [p;q;g] where for RLBench the gripper state is a single scalar open or closed [1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getdata import RLBenchDataset\n",
    "import torch\n",
    "\n",
    "\n",
    "# Read in the RLBench training data.  The RLBenchDataset class is a subclass of the PyTorch Dataset class.\n",
    "batch_size = 50\n",
    "\n",
    "# Load the training dataset and create a PyTorch DataLoader object.\n",
    "train_dataset = RLBenchDataset(\n",
    "    update=None,\n",
    "    dataset_name=\"reach_target\",\n",
    "    start_offset_ratio=None,\n",
    "    split=\"train\",\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(    \n",
    "    dataset=train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=0)\n",
    "\n",
    "# Load the validation dataset and create a PyTorch DataLoader object.\n",
    "val_dataset = RLBenchDataset(\n",
    "    update=None,\n",
    "    dataset_name=\"reach_target\",\n",
    "    start_offset_ratio=None,\n",
    "    split=\"val\",\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset=val_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset batch info:\n",
      "action: torch.Size([50, 3, 8])\n",
      "encoder_emb: torch.Size([50, 2, 768])\n",
      "target: torch.Size([50, 2, 8])\n",
      "num batches: 26\n",
      "Val dataset batch info:\n",
      "action: torch.Size([50, 3, 8])\n",
      "encoder_emb: torch.Size([50, 2, 768])\n",
      "target: torch.Size([50, 2, 8])\n",
      "num batches: 10\n",
      "torch.Size([50, 3, 8])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training dataset batch info:\")\n",
    "for i, (data, target) in enumerate(train_loader):\n",
    "    print(\"action:\", data['action'].shape)\n",
    "    print(\"encoder_emb:\", data['encoder_emb'].shape)\n",
    "    print(\"target:\", target.shape)\n",
    "    break\n",
    "print(\"num batches: \" + str(len(train_loader)))\n",
    "\n",
    "print(f\"Val dataset batch info:\")\n",
    "for i, (data, target) in enumerate(val_loader):\n",
    "    print(\"action:\", data['action'].shape)\n",
    "    print(\"encoder_emb:\", data['encoder_emb'].shape)\n",
    "    print(\"target:\", target.shape)\n",
    "    break\n",
    "print(\"num batches: \" + str(len(val_loader)))\n",
    "print(data['action'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, Tensor\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 500):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        # even indices are sine, odd indices are cosine\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[ batch_size, seq_len, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf],\n",
      "        [0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lk = nn.Transformer.generate_square_subsequent_mask(2)\n",
    "# plot the attention mask\n",
    "plt.imshow(lk.numpy())\n",
    "plt.show()\n",
    "print(lk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "\n",
    "class ActionDecoderModel(nn.Module):\n",
    "\n",
    "    def __init__(self, action_dim: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5, action_seq_len: int = 5,\n",
    "                 mem_seq_len: int = 5):\n",
    "        super().__init__()\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout=0, max_len=action_seq_len)\n",
    "        decoder_layer = TransformerDecoderLayer(d_model, nhead, d_hid, dropout, batch_first=True)\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layer, nlayers)\n",
    "        self.d_model = d_model\n",
    "        self.linear_action_in = nn.Linear(action_dim, d_model)\n",
    "        # The output of the transformer decoder is a sequence of length action_seq_len-1 because it doesn't have sos token\n",
    "        self.linear_action_out = nn.Linear(d_model, action_dim)\n",
    "        self.tgt_mask = nn.Transformer.generate_square_subsequent_mask(action_seq_len)\n",
    "        self.mem_mask = nn.Transformer.generate_square_subsequent_mask(mem_seq_len)\n",
    "   \n",
    "\n",
    "    def forward(self, actions: Tensor, memory: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        actions = self.linear_action_in(actions)\n",
    "        actions = self.pos_encoder(actions)\n",
    "        # If mixing tasks, then we will need padding masks in the batch\n",
    "        output = self.transformer_decoder(\n",
    "            tgt=actions, \n",
    "            memory=memory, \n",
    "            tgt_is_causal=True, \n",
    "            memory_is_causal=True,\n",
    "            tgt_mask=self.tgt_mask,\n",
    "            memory_mask=self.mem_mask\n",
    "            )\n",
    "        \n",
    "        output = self.linear_action_out(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# max number of tokens in encoder embeddings for the training tasks in a batch. Can be constant if all same task, otherwise needs to be computed. Also determines whether a padding mask must be applied.  Determines the size of the causal mask and max\n",
    "# max sequence length of the decoder action sequence and target output sequence. Decoder input size is seq_len + 2 to account for the start of sequence (sos) and end of sequence (eos) tokens.\n",
    "action_seq_len = 1 + 2  # sos + action + eos\n",
    "obs_seq_len = 2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "action_dim = 8 # feature length of the action vector [p;q;g]\n",
    "mm_dim = 768  # embedding dimension of the encoder (768,)\n",
    "d_hid = 768  # dimension of the feedforward network model in ``nn.TransformerDecoder``\n",
    "nlayers = 2  # number of nn.TransformerDecoderLayer in nn.TransformerDecoder\n",
    "nhead = 2  # number of attention heads in nn.MultiheadAttention\n",
    "dropout = 0.2  # dropout probability\n",
    "model = ActionDecoderModel(\n",
    "    action_dim=action_dim, \n",
    "    d_model = mm_dim, \n",
    "    nhead = nhead, \n",
    "    d_hid = d_hid, \n",
    "    nlayers = nlayers, \n",
    "    dropout = dropout, \n",
    "    action_seq_len = action_seq_len,\n",
    "    mem_seq_len = obs_seq_len,\n",
    "    ).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on cuda:0\n",
      "ActionDecoderModel(\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (transformer_decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "        (dropout3): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear_action_in): Linear(in_features=8, out_features=768, bias=True)\n",
      "  (linear_action_out): Linear(in_features=768, out_features=8, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# print whether model is on GPU or CPU\n",
    "print(f\"Model is on {next(model.parameters()).device}\")\n",
    "\n",
    "# Display the model architecture and number of trainable parameters\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-02.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "loss_fn = nn.MSELoss(\n",
    "    reduction='mean'\n",
    ")\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.01,\n",
    "    betas=(0.9, 0.999),\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "    optimizer, \n",
    "    gamma=0.9,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "epochs = 10 # The number of epochs\n",
    "# Create a 2D array of zeros to store the training loss for each epoch.\n",
    "train_loss_buf = np.zeros((epochs, len(train_loader)))\n",
    "\n",
    "def train(model: nn.Module) -> None:\n",
    "    model.train()\n",
    "    log_interval_loss = 0.\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i, (batch, targets) in enumerate(train_loader):\n",
    "        \n",
    "        encoder_embeddings = batch['encoder_emb'].cuda()\n",
    "        action_inputs = batch['action'].cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(actions=action_inputs, memory=encoder_embeddings)\n",
    "        targets = torch.squeeze(targets).cuda()\n",
    "        # Compare target sequence to output sequence, ignoring the sos token\n",
    "        batch_train_loss = loss_fn(outputs[:,1:,:], targets)\n",
    "        batch_train_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        log_interval_loss += batch_train_loss.item()\n",
    "        train_loss_buf[epoch-1, i] = batch_train_loss.item()\n",
    "        log_interval = 10\n",
    "        if i % log_interval == 0 and i >= 0:\n",
    "            cur_loss = log_interval_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f'| epoch: {epoch:3d} | {i+1:5d}/{len(train_loader):5d} batches | '\n",
    "                  f'lr: {scheduler.get_last_lr()[0]:02.3f} | ms/batch: {elapsed * 1000 / log_interval:5.2f} | '\n",
    "                  f'log batch loss: {log_interval_loss:1.5f} | ')\n",
    "            log_interval_loss = 0\n",
    "            start_time = time.time()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate(model: nn.Module, val_loader: iter) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_val_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for i, (batch, targets) in enumerate(train_loader):\n",
    "            encoder_embeddings = batch['mm_embeddings'].cuda()\n",
    "            action_inputs = batch['action'].cuda()\n",
    "            targets = torch.squeeze(targets).cuda()\n",
    "            output = model(actions=action_inputs, memory=encoder_embeddings)\n",
    "            batch_val_loss = loss_fn(output, targets)\n",
    "            total_val_loss += batch_val_loss.item()\n",
    "            \n",
    "    return np.mean(total_val_loss, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch:   1 |     1/   26 batches | lr: 0.010 | ms/batch: 55.09 | log batch loss: 0.52343 | \n",
      "| epoch:   1 |    11/   26 batches | lr: 0.010 | ms/batch: 112.71 | log batch loss: 53.24119 | \n",
      "| epoch:   1 |    21/   26 batches | lr: 0.010 | ms/batch: 107.91 | log batch loss: 9.50702 | \n",
      "Adjusting learning rate of group 0 to 9.0000e-03.\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch:   1 | epoc time:  3.23s | \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch:   2 |     1/   26 batches | lr: 0.009 | ms/batch: 10.79 | log batch loss: 0.60272 | \n",
      "| epoch:   2 |    11/   26 batches | lr: 0.009 | ms/batch: 114.30 | log batch loss: 5.00982 | \n",
      "| epoch:   2 |    21/   26 batches | lr: 0.009 | ms/batch: 108.10 | log batch loss: 4.19241 | \n",
      "Adjusting learning rate of group 0 to 8.1000e-03.\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch:   2 | epoc time:  2.82s | \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch:   3 |     1/   26 batches | lr: 0.008 | ms/batch: 11.05 | log batch loss: 0.36504 | \n",
      "| epoch:   3 |    11/   26 batches | lr: 0.008 | ms/batch: 105.83 | log batch loss: 3.65218 | \n",
      "| epoch:   3 |    21/   26 batches | lr: 0.008 | ms/batch: 108.61 | log batch loss: 3.34324 | \n",
      "Adjusting learning rate of group 0 to 7.2900e-03.\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch:   3 | epoc time:  2.76s | \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch:   4 |     1/   26 batches | lr: 0.007 | ms/batch: 10.52 | log batch loss: 0.33900 | \n",
      "| epoch:   4 |    11/   26 batches | lr: 0.007 | ms/batch: 108.39 | log batch loss: 3.40006 | \n",
      "| epoch:   4 |    21/   26 batches | lr: 0.007 | ms/batch: 111.12 | log batch loss: 3.26627 | \n",
      "Adjusting learning rate of group 0 to 6.5610e-03.\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch:   4 | epoc time:  2.76s | \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch:   5 |     1/   26 batches | lr: 0.007 | ms/batch: 10.33 | log batch loss: 0.31894 | \n",
      "| epoch:   5 |    11/   26 batches | lr: 0.007 | ms/batch: 107.87 | log batch loss: 3.13453 | \n",
      "| epoch:   5 |    21/   26 batches | lr: 0.007 | ms/batch: 110.09 | log batch loss: 3.25284 | \n",
      "Adjusting learning rate of group 0 to 5.9049e-03.\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch:   5 | epoc time:  2.76s | \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch:   6 |     1/   26 batches | lr: 0.006 | ms/batch: 10.97 | log batch loss: 0.31664 | \n",
      "| epoch:   6 |    11/   26 batches | lr: 0.006 | ms/batch: 107.39 | log batch loss: 3.36153 | \n",
      "| epoch:   6 |    21/   26 batches | lr: 0.006 | ms/batch: 111.22 | log batch loss: 3.08899 | \n",
      "Adjusting learning rate of group 0 to 5.3144e-03.\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch:   6 | epoc time:  2.77s | \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch:   7 |     1/   26 batches | lr: 0.005 | ms/batch: 10.29 | log batch loss: 0.30528 | \n",
      "| epoch:   7 |    11/   26 batches | lr: 0.005 | ms/batch: 108.03 | log batch loss: 3.14356 | \n",
      "| epoch:   7 |    21/   26 batches | lr: 0.005 | ms/batch: 107.85 | log batch loss: 3.14373 | \n",
      "Adjusting learning rate of group 0 to 4.7830e-03.\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch:   7 | epoc time:  2.72s | \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch:   8 |     1/   26 batches | lr: 0.005 | ms/batch: 10.57 | log batch loss: 0.29307 | \n",
      "| epoch:   8 |    11/   26 batches | lr: 0.005 | ms/batch: 106.58 | log batch loss: 3.03290 | \n",
      "| epoch:   8 |    21/   26 batches | lr: 0.005 | ms/batch: 109.47 | log batch loss: 2.97698 | \n",
      "Adjusting learning rate of group 0 to 4.3047e-03.\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch:   8 | epoc time:  2.75s | \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch:   9 |     1/   26 batches | lr: 0.004 | ms/batch: 10.65 | log batch loss: 0.31225 | \n",
      "| epoch:   9 |    11/   26 batches | lr: 0.004 | ms/batch: 108.16 | log batch loss: 2.99867 | \n",
      "| epoch:   9 |    21/   26 batches | lr: 0.004 | ms/batch: 112.92 | log batch loss: 3.02344 | \n",
      "Adjusting learning rate of group 0 to 3.8742e-03.\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch:   9 | epoc time:  2.88s | \n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch:  10 |     1/   26 batches | lr: 0.004 | ms/batch: 11.42 | log batch loss: 0.31115 | \n",
      "| epoch:  10 |    11/   26 batches | lr: 0.004 | ms/batch: 109.57 | log batch loss: 3.03555 | \n",
      "| epoch:  10 |    21/   26 batches | lr: 0.004 | ms/batch: 112.66 | log batch loss: 2.95962 | \n",
      "Adjusting learning rate of group 0 to 3.4868e-03.\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch:  10 | epoc time:  2.82s | \n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(model)\n",
    "       # val_loss = evaluate(model, val_loader)\n",
    "        scheduler.step()\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        print('-' * 89)\n",
    "        print(f'| end of epoch: {epoch:3d} | epoc time: {elapsed:5.2f}s | ')\n",
    "        print('-' * 89)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    print(f'| end of epoch: {epoch:3d} | epoc time: {elapsed:5.2f}s | '\n",
    "     #         f'valid loss: {val_loss:5.2f} | valid ppl: {np.exp(np.min([val_loss,10])):8.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot the training loss for each epoch\n",
    "plt.plot(np.log(np.transpose(train_loss_buf)))\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Log Training loss')\n",
    "\n",
    "# add a legend\n",
    "plt.legend(['Epoch 1', 'Epoch 2', 'Epoch 3', 'Epoch 4', 'Epoch 5'])\n",
    "plt.show()\n",
    "\n",
    "# save the image to disk with the current date in the name of the file to the /home/levi directory\n",
    "today = date.today()\n",
    "fig1 = plt.gcf()\n",
    "#fig1.savefig(f'/home/levi/transformer_training_loss_{today}.jpg')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
