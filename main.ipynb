{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bring in the RLBench BC data from the saved hdf5 data files for each task. The task files were created with the instructRL/data/collect_data.py script. The task files include an array of data samples.  Each sample is a dict that includes the following keys:\n",
    "\n",
    "* image - data for 4 camera positions: front_rgb, left_shoulder_rgb, right_shoulder_rgb, wrist_rgb\n",
    "* instruct - the text instruction\n",
    "* action - the action vector: [p;q;g] where for RLBench the gripper state is a single scalar open or closed [1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getdata import RLBenchDataset\n",
    "import torch\n",
    "\n",
    "\n",
    "# Read in the RLBench training data.  The RLBenchDataset class is a subclass of the PyTorch Dataset class.\n",
    "batch_size = 2\n",
    "\n",
    "# Load the training dataset and create a PyTorch DataLoader object.\n",
    "train_dataset = RLBenchDataset(\n",
    "    update=None,\n",
    "    dataset_name=\"reach_target\",\n",
    "    start_offset_ratio=None,\n",
    "    split=\"train\",\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(    \n",
    "    dataset=train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=0)\n",
    "\n",
    "# Load the validation dataset and create a PyTorch DataLoader object.\n",
    "val_dataset = RLBenchDataset(\n",
    "    update=None,\n",
    "    dataset_name=\"reach_target\",\n",
    "    start_offset_ratio=None,\n",
    "    split=\"val\",\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset=val_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training dataset batch info:\")\n",
    "for i, (data, target) in enumerate(train_loader):\n",
    "    print(\"data:\", data['action'].shape)\n",
    "    print(\"target:\", target.shape)\n",
    "    break\n",
    "print(\"num batches: \" + str(len(train_loader)))\n",
    "\n",
    "print(f\"Val dataset batch info:\")\n",
    "for i, (data, target) in enumerate(val_loader):\n",
    "    print(\"data:\", data['action'].shape)\n",
    "    print(\"target:\", target.shape)\n",
    "    break\n",
    "print(\"num batches: \" + str(len(val_loader)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, Tensor\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "\n",
    "class ActionDecoderModel(nn.Module):\n",
    "\n",
    "    def __init__(self, action_dim: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5, action_seq_len: int = 4,\n",
    "                 mem_seq_len: int = 10):\n",
    "        super().__init__()\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout=0, max_len=action_seq_len)\n",
    "        decoder_layer = TransformerDecoderLayer(d_model, nhead, d_hid, dropout, batch_first=True)\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layer, nlayers)\n",
    "        self.d_model = d_model\n",
    "        self.linear_action_in = nn.Linear(action_dim, d_model)\n",
    "        self.linear_action_out = nn.Linear(action_seq_len*d_model, action_dim)\n",
    "        self.tgt_mask = nn.Transformer.generate_square_subsequent_mask(action_seq_len)\n",
    "        self.mem_mask = nn.Transformer.generate_square_subsequent_mask(mem_seq_len)\n",
    "   \n",
    "\n",
    "    def forward(self, actions: Tensor, memory: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        actions = self.linear_action_in(actions)\n",
    "        actions = self.pos_encoder(actions)\n",
    "        output = self.transformer_decoder(\n",
    "            tgt=actions, \n",
    "            memory=memory, \n",
    "            tgt_is_causal=True, \n",
    "            memory_is_causal=True,\n",
    "            tgt_mask=self.tgt_mask,\n",
    "            memory_mask=self.mem_mask\n",
    "            )\n",
    "        output = output.reshape(output.shape[0], -1)\n",
    "        output = self.linear_action_out(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seq_len = 4\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "action_dim = 8 # length of the action vector \n",
    "mm_dim = 768  # embedding dimension\n",
    "d_hid = 768  # dimension of the feedforward network model in ``nn.TransformerDecoder``\n",
    "nlayers = 2  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerDecoder``\n",
    "nhead = 2  # number of heads in ``nn.MultiheadAttention``\n",
    "dropout = 0.2  # dropout probability\n",
    "model = ActionDecoderModel(\n",
    "    action_dim=action_dim, \n",
    "    d_model = mm_dim, \n",
    "    nhead = nhead, \n",
    "    d_hid = d_hid, \n",
    "    nlayers = nlayers, \n",
    "    dropout = dropout, \n",
    "    action_seq_len = seq_len-1, \n",
    "    mem_seq_len = seq_len\n",
    "    ).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print whether model is on GPU or CPU\n",
    "print(f\"Model is on {next(model.parameters()).device}\")\n",
    "\n",
    "# Display the model architecture and number of trainable parameters\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "loss_fn = nn.MSELoss(\n",
    "    reduction='mean'\n",
    ")\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.01,\n",
    "    betas=(0.9, 0.999),\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "    optimizer, \n",
    "    gamma=0.9,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Create a 2D array of zeros to store the training loss for each epoch.\n",
    "train_loss_buf = np.zeros((5, len(train_loader)))\n",
    "\n",
    "def train(model: nn.Module) -> None:\n",
    "    model.train()\n",
    "    log_interval_loss = 0.\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i, (batch, targets) in enumerate(train_loader):\n",
    "        \n",
    "        encoder_embeddings = batch['mm_embeddings'].cuda()\n",
    "        action_inputs = batch['action'].cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(actions=action_inputs, memory=encoder_embeddings)\n",
    "        targets = torch.squeeze(targets).cuda()\n",
    "        batch_train_loss = loss_fn(output, targets)\n",
    "        batch_train_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        log_interval_loss += batch_train_loss.item()\n",
    "        train_loss_buf[epoch, i] = batch_train_loss.item()\n",
    "        log_interval = 1\n",
    "        if i % log_interval == 0 and i >= 0:\n",
    "            cur_loss = log_interval_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f'| epoch: {epoch:3d} | {i+1:5d}/{len(train_loader):5d} batches | '\n",
    "                  f'lr: {scheduler.get_last_lr()[0]:02.3f} | ms/batch: {elapsed * 1000 / log_interval:5.2f} | '\n",
    "                  f'log int loss: {log_interval_loss:5.2f} | ')\n",
    "            log_interval_loss = 0\n",
    "            start_time = time.time()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate(model: nn.Module, val_loader: iter) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_val_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for i, (batch, targets) in enumerate(train_loader):\n",
    "            encoder_embeddings = batch['mm_embeddings'].cuda()\n",
    "            action_inputs = batch['action'].cuda()\n",
    "            targets = torch.squeeze(targets).cuda()\n",
    "            output = model(actions=action_inputs, memory=encoder_embeddings)\n",
    "            batch_val_loss = loss_fn(output, targets)\n",
    "            total_val_loss += batch_val_loss.item()\n",
    "            \n",
    "    return np.mean(total_val_loss, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5 # The number of epochs\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(model)\n",
    "       # val_loss = evaluate(model, val_loader)\n",
    "        scheduler.step()\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        print('-' * 89)\n",
    "        print(f'| end of epoch: {epoch:3d} | epoc time: {elapsed:5.2f}s | ')\n",
    "        print('-' * 89)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    print(f'| end of epoch: {epoch:3d} | epoc time: {elapsed:5.2f}s | '\n",
    "     #         f'valid loss: {val_loss:5.2f} | valid ppl: {np.exp(np.min([val_loss,10])):8.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot the training loss for each epoch\n",
    "plt.plot(np.log(np.transpose(train_loss_buf)))\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Log Training loss')\n",
    "\n",
    "# add a legend\n",
    "plt.legend(['Epoch 1', 'Epoch 2', 'Epoch 3', 'Epoch 4', 'Epoch 5'])\n",
    "plt.show()\n",
    "\n",
    "# save the image to disk with the current date in the name of the file to the /home/levi directory\n",
    "today = date.today()\n",
    "fig1 = plt.gcf()\n",
    "fig1.savefig(f'/home/levi/transformer_training_loss_{today}.jpg')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
