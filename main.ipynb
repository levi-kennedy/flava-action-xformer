{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bring in the RLBench BC data from the saved hdf5 data files for each task. The task files were created with the instructRL/data/collect_data.py script. The task files include an array of data samples.  Each sample is a dict that includes the following keys:\n",
    "\n",
    "* image - data for 4 camera positions: front_rgb, left_shoulder_rgb, right_shoulder_rgb, wrist_rgb\n",
    "* instruct - the text instruction\n",
    "* action - the action vector: [p;q;g] where for RLBench the gripper state is a single scalar open or closed [1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "from getdata import RLBenchDataset\n",
    "import time\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pprint as pp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch import Tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 500):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        # even indices are sine, odd indices are cosine\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[ batch_size, seq_len, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EncoderAdapter(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Assuming the input is of shape [batch_size, channels=3, seq_length=396, features=768]\n",
    "        self.conv2_1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2_3 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(3, stride=3)\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        self.linear = nn.Linear(16*14*28, 768)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"Input shape: {x.shape}\")\n",
    "        x = F.relu(self.conv2_1(x))  # Reducing the sequence length [batch_size, 16, 396, 768]]\n",
    "        print(f\"Conv2_1 shape: {x.shape}\")\n",
    "        x = self.pool(x) # [batch_size, 16, 132, 256]\n",
    "        print(f\"Pool shape: {x.shape}\")\n",
    "        x = F.relu(self.conv2_2(x)) # [batch_size, 16, 132, 256]\n",
    "        print(f\"Conv2_2 shape: {x.shape}\")\n",
    "        x = self.pool(x) # [batch_size, 16, 44, 85]\n",
    "        print(f\"Pool shape: {x.shape}\")\n",
    "        x = F.relu(self.conv2_3(x)) # [batch_size, 16, 44, 85]\n",
    "        print(f\"Conv2_3 shape: {x.shape}\")\n",
    "        x = self.pool(x) # [batch_size, 16, 14, 28]\n",
    "        print(f\"Pool shape: {x.shape}\")\n",
    "        x = self.flatten(x)  # Flatten the tensor to [batch_size, 16*14*28]\n",
    "        print(f\"Flatten shape: {x.shape}\")\n",
    "        x = self.linear(x)  # Get the final desired shape\n",
    "        print(f\"Linear shape: {x.shape}\")\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ActionDecoderModel(nn.Module):\n",
    "\n",
    "    def __init__(self, action_dim: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.2, action_seq_len: int = 5,\n",
    "                 mem_seq_len: int = 5, test_mode=False):\n",
    "        super().__init__()\n",
    "        #self.pos_encoder = PositionalEncoding(d_model, dropout=0, max_len=action_seq_len).cuda()\n",
    "        self.encoder_adapter = EncoderAdapter().cuda()\n",
    "        #self.encoder_adapter = nn.Linear(action_dim, d_model).cuda()\n",
    "        decoder_layer = TransformerDecoderLayer(d_model, nhead, d_hid, dropout, batch_first=True).cuda()\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layer, nlayers).cuda()\n",
    "        self.d_model = d_model\n",
    "        self.linear_action_in = nn.Linear(action_dim, d_model).cuda()\n",
    "        # The output of the transformer decoder is a sequence of length action_seq_len-1 because it doesn't have sos token\n",
    "        self.linear_action_out = nn.Linear(d_model, action_dim).cuda()\n",
    "        self.test_mode = test_mode\n",
    "        \n",
    "\n",
    "    def forward(self, actions: Tensor, memory: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        actions: [batch_size, action_seq_len, action_dim]\n",
    "        memory: [batch_size, n_images, mem_seq_len, d_model]\n",
    "        \"\"\"\n",
    "        actions = self.linear_action_in(actions)\n",
    "        # actions = self.pos_encoder(actions)\n",
    "        memory = memory.mean(dim=-2)\n",
    "        memory = memory.reshape(memory.size(0), -1, memory.size(-1))\n",
    "        memory = memory[:,0:3,:] # Only do this for ReachTarget otherwise need to implement smart train memory mask\n",
    "        if self.test_mode:\n",
    "            # If mixing tasks, then we will need padding masks in the batch\n",
    "            output = self.transformer_decoder(\n",
    "                tgt=actions, \n",
    "                memory=memory\n",
    "                )\n",
    "            \n",
    "        else:\n",
    "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(actions.size(1)).cuda()\n",
    "            # causal memory mask to prevent attending to future actions of size mem_seq_len x action_seq_len upper triangular part should be True (is masked)\n",
    "            mem_mask = torch.triu(torch.ones(actions.size(1), memory.size(1)), diagonal=1).bool().cuda()\n",
    "            \n",
    "            # If mixing tasks, then we will need padding masks in the batch\n",
    "            output = self.transformer_decoder(\n",
    "                tgt=actions, \n",
    "                memory=memory, \n",
    "                tgt_mask=tgt_mask,\n",
    "                )\n",
    "\n",
    "\n",
    "        output = self.linear_action_out(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The encoder embeddings from the Flava encoder will have dimensions [batch size, seq length, num cameras, num tokens, embedding dim]\n",
    "x = torch.rand(5, 2, 3, 396, 768)\n",
    "# Apply 1D ave pooling to the second to last dimension\n",
    "x = x.mean(dim=-2)\n",
    "x = x.reshape(x.size(0), -1, x.size(-1))\n",
    "print(f\"Input shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to open file (unable to open file: name = '/home/levi/data/flavaActionDecoderData/reach_target_train.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m task_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreach_target\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load the training dataset and create a PyTorch DataLoader object.\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mRLBenchDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupdate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_offset_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(    \n\u001b[1;32m     15\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mtrain_dataset, \n\u001b[1;32m     16\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size, \n\u001b[1;32m     17\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m     18\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Load the validation dataset and create a PyTorch DataLoader object.\u001b[39;00m\n",
      "File \u001b[0;32m~/code/flava-action-xformer/getdata.py:74\u001b[0m, in \u001b[0;36mRLBenchDataset.__init__\u001b[0;34m(self, update, dataset_name, start_offset_ratio, split)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown split: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Open the hdf5 file where the data is stored\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh5_file \u001b[38;5;241m=\u001b[39m \u001b[43mh5py\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrandom_start:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_start_offset \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mdefault_rng()\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))\n",
      "File \u001b[0;32m~/code/flava-action-xformer/.venv/lib/python3.11/site-packages/h5py/_hl/files.py:567\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    558\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    559\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    560\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[1;32m    561\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[1;32m    562\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[1;32m    563\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    564\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    565\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    566\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 567\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m~/code/flava-action-xformer/.venv/lib/python3.11/site-packages/h5py/_hl/files.py:231\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[1;32m    230\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 231\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    233\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:106\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to open file (unable to open file: name = '/home/levi/data/flavaActionDecoderData/reach_target_train.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "\n",
    "### DON'T FORGET to change the number of workers in the dataloader to 0 if debugging, debugger doesn't like mulitiple threads  ###\n",
    "\n",
    "# Read in the RLBench training data.  The RLBenchDataset class is a subclass of the PyTorch Dataset class.\n",
    "batch_size = 10\n",
    "task_name = \"reach_target\"\n",
    "# Load the training dataset and create a PyTorch DataLoader object.\n",
    "train_dataset = RLBenchDataset(\n",
    "    update=None,\n",
    "    dataset_name=task_name,\n",
    "    start_offset_ratio=None,\n",
    "    split=\"train\",\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(    \n",
    "    dataset=train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=0)\n",
    "\n",
    "# Load the validation dataset and create a PyTorch DataLoader object.\n",
    "val_dataset = RLBenchDataset(\n",
    "    update=None,\n",
    "    dataset_name=task_name,\n",
    "    start_offset_ratio=None,\n",
    "    split=\"val\",\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset=val_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training dataset batch info:\")\n",
    "for i, (data, target) in enumerate(train_loader):\n",
    "    print(\"action:\", data['action'].shape)\n",
    "    print(\"encoder_emb:\", data['encoder_emb'].shape)\n",
    "    print(\"target:\", target.shape)\n",
    "    break\n",
    "print(\"num batches: \" + str(len(train_loader)))\n",
    "\n",
    "print(f\"Val dataset batch info:\")\n",
    "for i, (data, target) in enumerate(val_loader):\n",
    "    print(\"action:\", data['action'].shape)\n",
    "    print(\"encoder_emb:\", data['encoder_emb'].shape)\n",
    "    print(\"target:\", target.shape)\n",
    "    break\n",
    "print(\"num batches: \" + str(len(val_loader)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# max number of tokens in encoder embeddings for the training tasks in a batch. Can be constant if all same task, otherwise needs to be computed. Also determines whether a padding mask must be applied.  Determines the size of the causal mask and max\n",
    "# max sequence length of the decoder action sequence and target output sequence. Decoder input size is seq_len + 2 to account for the start of sequence (sos) and end of sequence (eos) tokens.\n",
    "action_seq_len = data['action'].shape[1]  # sos + action + eos, will need batch padding when mixing tasks\n",
    "mem_seq_len = data['encoder_emb'].shape[1] \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "action_dim = 8 # feature length of the action vector [p;q;g]\n",
    "mm_dim = 768  # embedding dimension of the encoder (768,)\n",
    "d_hid = 768  # dimension of the feedforward network model in ``nn.TransformerDecoder``\n",
    "nlayers = 4  # number of nn.TransformerDecoderLayer in nn.TransformerDecoder\n",
    "nhead = 4  # number of attention heads in nn.MultiheadAttention\n",
    "dropout = 0.0  # dropout probability\n",
    "model = ActionDecoderModel(\n",
    "    action_dim=action_dim, \n",
    "    d_model = mm_dim, \n",
    "    nhead = nhead, \n",
    "    d_hid = d_hid, \n",
    "    nlayers = nlayers, \n",
    "    dropout = dropout, \n",
    "    action_seq_len = action_seq_len,\n",
    "    mem_seq_len = mem_seq_len,\n",
    "    ).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print whether model is on GPU or CPU\n",
    "print(f\"Model is on {next(model.parameters()).device}\")\n",
    "\n",
    "# Display the model architecture and number of trainable parameters\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "loss_fn = nn.MSELoss(\n",
    "    reduction='mean',    \n",
    ")\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.001,\n",
    "    betas=(0.9, 0.999),\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "    optimizer, \n",
    "    gamma=0.99,\n",
    "    verbose=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# # Create a 2D array of zeros to store the training loss for each epoch x batch.\n",
    "# train_loss_buf = np.zeros((epochs, len(train_loader)))\n",
    "# def train(model: nn.Module) -> None:\n",
    "    \n",
    "#     model.train()\n",
    "#     log_interval_loss = 0.\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     for i, (batch, targets) in enumerate(train_loader):\n",
    "        \n",
    "#         encoder_embeddings = batch['encoder_emb'].cuda()\n",
    "#         action_inputs = batch['action'].cuda()\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(actions=action_inputs, memory=encoder_embeddings)        \n",
    "#         targets = torch.squeeze(targets).cuda()\n",
    "#         # print the elements of outputs and targets side by side to compare the values\n",
    "#         print(f\"train: outputs: {outputs}: targets: {targets}\")\n",
    "       \n",
    "\n",
    "\n",
    "#         # Compare target sequence to output sequence, ignoring the output token after the eos token\n",
    "#         batch_train_loss = loss_fn(outputs[:,:-1,:], targets)\n",
    "#         batch_train_loss.backward()\n",
    "#         #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "#         optimizer.step()\n",
    "\n",
    "#         log_interval_loss += batch_train_loss.item()\n",
    "#         train_loss_buf[epoch, i] = batch_train_loss.item()\n",
    "#         log_interval = 10\n",
    "#         if i % log_interval == 0 and i > 0:\n",
    "#             cur_loss = log_interval_loss / log_interval\n",
    "#             elapsed = time.time() - start_time\n",
    "#             # print(f'| epoch: {epoch:3d} | {i+1:5d}/{len(train_loader):5d} batches | '\n",
    "#             #       f'lr: {scheduler.get_last_lr()[0]:02.3f} | ms/batch: {elapsed * 1000 / log_interval:5.2f} | '\n",
    "#             #       f'log batch loss: {log_interval_loss:1.5f} | ')\n",
    "#             log_interval_loss = 0\n",
    "#             start_time = time.time()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "val_loss_buf = np.zeros((epochs, len(val_loader)))\n",
    "def evaluate(model: nn.Module, val_loader: iter) -> float:\n",
    "    \n",
    "    model.eval()  # turn on evaluation mode    \n",
    "    with torch.no_grad():\n",
    "        for i, (batch, targets) in enumerate(val_loader):\n",
    "            encoder_embeddings = batch['encoder_emb'].cuda()\n",
    "            action_inputs = batch['action'].cuda()\n",
    "            targets = torch.squeeze(targets).cuda()\n",
    "            outputs = model(actions=action_inputs, memory=encoder_embeddings)\n",
    "            # print the output and target values side by side to compare\n",
    "            #print(f\"val: outputs: {outputs}: targets: {targets}\")\n",
    "            batch_val_loss = loss_fn(outputs, targets)\n",
    "            val_loss_buf[epoch, i] = batch_val_loss.item()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=4, sci_mode=False, linewidth=150)\n",
    "epochs = 1 # The number of epochs\n",
    "\n",
    "# Create a 2D array of zeros to store the training loss for each epoch x batch.\n",
    "train_loss_buf = np.zeros((epochs, len(train_loader)))\n",
    "for epoch in range(epochs):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "\n",
    "    log_interval_loss = 0.\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i, (batch, targets) in enumerate(train_loader):\n",
    "\n",
    "        encoder_embeddings = batch['encoder_emb'].cuda()\n",
    "        action_inputs = batch['action'].cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(actions=action_inputs, memory=encoder_embeddings)        \n",
    "        targets = torch.squeeze(targets).cuda()\n",
    "        # print the elements of outputs and targets side by side to compare the values\n",
    "        # print(f\"train: outputs: {outputs}| targets: {targets}\")\n",
    "\n",
    "\n",
    "\n",
    "        # Compare target sequence to output sequence, ignoring the output token after the eos token\n",
    "        batch_train_loss = loss_fn(outputs, targets)\n",
    "        batch_train_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        log_interval_loss += batch_train_loss.item()\n",
    "        train_loss_buf[epoch, i] = batch_train_loss.item()\n",
    "        log_interval = 10\n",
    "        if i % log_interval == 0 and i > 0:\n",
    "                cur_loss = log_interval_loss / log_interval\n",
    "                elapsed = time.time() - start_time\n",
    "                # print(f'| epoch: {epoch:3d} | {i+1:5d}/{len(train_loader):5d} batches | '\n",
    "                #       f'lr: {scheduler.get_last_lr()[0]:02.3f} | ms/batch: {elapsed * 1000 / log_interval:5.2f} | '\n",
    "                #       f'log batch loss: {log_interval_loss:1.5f} | ')\n",
    "                log_interval_loss = 0\n",
    "                start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    evaluate(model, val_loader)\n",
    "    scheduler.step()\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print('-' * 89)\n",
    "    print(f'end of epoch: {epoch:3d} | epoc time: {elapsed:5.2f}s ')\n",
    "    # print the total training and validation loss for the epoch\n",
    "    print(f'train loss: {train_loss_buf[epoch,:].mean():6.7f}')              \n",
    "    print(f'valid loss: {val_loss_buf[epoch,:].mean():6.7f}')\n",
    "    print(f'Learning Rate: {scheduler.get_last_lr()[0]:02.6f}')\n",
    "    print('-' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_MODEL = False\n",
    "\n",
    "# Model path with date suffix\n",
    "datestr = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "MODEL_PATH = f\"/home/levi/data/action_decoder_model_{datestr}.pt\"\n",
    "if SAVE_MODEL:\n",
    "    torch.save(model, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training loss for each epoch\n",
    "\n",
    "plt.plot(np.vstack((np.mean(train_loss_buf, axis=1), np.mean(val_loss_buf, axis=1))).T)\n",
    "# set the x axis tick labels to be the epoch numbers\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Ave Loss')\n",
    "plt.xticks(np.arange(0, epochs, step=25))\n",
    "\n",
    "# add a legend\n",
    "plt.legend(['Training Loss', 'Validation Loss'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(np.log(np.vstack((np.mean(train_loss_buf, axis=1), np.mean(val_loss_buf, axis=1))).T))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Ave Log-Loss')\n",
    "plt.xticks(np.arange(0, epochs, step=25))\n",
    "\n",
    "\n",
    "# add a legend\n",
    "plt.legend(['Training Loss', 'Validation Loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets connect the current model to RLBench to step through the task and evaluate how often it receives a reward in the task.\n",
    "from rlbench.action_modes.action_mode import MoveArmThenGripper\n",
    "from rlbench.action_modes.arm_action_modes import EndEffectorPoseViaPlanning\n",
    "from rlbench.action_modes.gripper_action_modes import Discrete\n",
    "from rlbench.environment import Environment\n",
    "from rlbench.observation_config import ObservationConfig\n",
    "from rlbench.tasks import FS10_V1, ReachTarget, PickUpCup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import FlavaProcessor, FlavaModel\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class SlowAgent(object):\n",
    "    \"\"\"\n",
    "    Agent that uses task description and observation to infer FLAVA autoencoder embeddings and then \n",
    "    predict the next action with the provided decoder autoregressively.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, decoder_model):\n",
    "        self.action_shape = (8,)        \n",
    "        self.encoder_emb = np.empty((1,0,3,395,768),dtype=np.float32)\n",
    "        \n",
    "        # begin with the sos token\n",
    "        sos = torch.from_numpy(np.zeros((1,1,self.action_shape[0]), dtype=np.float32))\n",
    "        sos[0,0,0::2] = -1 # even values are -1\n",
    "        sos = sos/np.linalg.norm(sos)\n",
    "        self.decoder_actions = sos.cuda()\n",
    "        # Retrieve the Flava model and processor\n",
    "        self.flava_model = FlavaModel.from_pretrained('facebook/flava-full')\n",
    "        self.flava_processor = FlavaProcessor.from_pretrained('facebook/flava-full')\n",
    "        self.action_decoder_model = decoder_model\n",
    "\n",
    "\n",
    "    def get_flava_embeddings(self, images, instruction):\n",
    "        # Convert the observation and instruction into a batch of inputs for the Flava model\n",
    "        instruction = [instruction for n in range(len(images))] # repeat the instruction for each image\n",
    "        inputs = self.flava_processor(images, instruction, return_tensors=\"pt\", padding=\"max_length\", max_length=197, return_codebook_pixels=False, return_image_mask=False)\n",
    "        # Pass the inputs through the Flava model\n",
    "        outputs = self.flava_model(**inputs)\n",
    "        # Retrieve the multimodal embeddings from the Flava model outputs\n",
    "        multimodal_embeddings = outputs.multimodal_embeddings.detach().numpy()\n",
    "        return multimodal_embeddings\n",
    "    \n",
    "    def act(self, images, instruction):        \n",
    "        # Here we are submitting on image and one instruction at a time in an autoregressive fashion\n",
    "        encoder_emb = self.get_flava_embeddings(images, instruction)\n",
    "        #add axes to encoder_emb to match the expected shape of the self.encoder_emb\n",
    "        encoder_emb = np.expand_dims(encoder_emb, axis=(0,1))\n",
    "        # Concatenate the embedding to the encoder embeddings\n",
    "        self.encoder_emb = np.concatenate((self.encoder_emb, encoder_emb), axis=1)\n",
    "\n",
    "        self.action_decoder_model.eval()  # turn on evaluation mode    \n",
    "        memory_emb = torch.from_numpy(self.encoder_emb).cuda()\n",
    "        with torch.no_grad():\n",
    "            # Get the decoder action from the action decoder model\n",
    "            decoder_action = self.action_decoder_model(actions=self.decoder_actions, memory=memory_emb) \n",
    "            # Normalize the quaternion values\n",
    "            decoder_action[0,0,3:7] = decoder_action[0,0,3:7] / torch.norm(decoder_action[0,0,3:7])           \n",
    "            # Add the action to the decoder actions\n",
    "            self.decoder_actions = torch.cat((self.decoder_actions, decoder_action), dim=1)\n",
    "        return decoder_action.detach().cpu().numpy().squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_config = ObservationConfig()\n",
    "obs_config.set_all(True)\n",
    "\n",
    "env = Environment(\n",
    "    action_mode=MoveArmThenGripper(\n",
    "        arm_action_mode=EndEffectorPoseViaPlanning(absolute_mode=True), gripper_action_mode=Discrete()),\n",
    "    obs_config=ObservationConfig(),\n",
    "    headless=False)\n",
    "env.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the agent\n",
    "model.test_mode = True\n",
    "agent = SlowAgent(decoder_model=model)\n",
    "# Get the task\n",
    "task = env.get_task(ReachTarget)\n",
    "# Reset the task\n",
    "descriptions, obs = task.reset()\n",
    "instruction = descriptions[1]\n",
    "print(instruction)\n",
    "plt.imshow(obs.front_rgb)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take steps in the task and print the reward\n",
    "for i in range(1):\n",
    "    action = agent.act([obs.left_shoulder_rgb, obs.right_shoulder_rgb, obs.wrist_rgb], instruction)\n",
    "    print(f'action: {action}')\n",
    "    if action[0] > 0:\n",
    "        obs, reward, terminate = task.step(action)\n",
    "        print(f'Step: {i} Reward: {reward}')\n",
    "    else:\n",
    "        print(\"No action taken\")\n",
    "        break\n",
    "    plt.imshow(obs.front_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
